{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSC 80: Homework 04\n",
    "\n",
    "### Due Date: Monday, Feb 04 12:00PM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "Much like in DSC 10, this Jupyter Notebook contains the statements of the homework problems and provides code and markdown cells to display your answers to the problems. Unlike DSC 10, the notebook is *only* for displaying a readable version of your final answers. The coding work will be developed in an accompanying `hw0X.py` file, that will be imported into the current notebook. (`X` is a homework number)\n",
    "\n",
    "Homeworks and programming assignments will be graded in (at most) two ways:\n",
    "1. The functions and classes in the accompanying python file will be tested (a la DSC 20),\n",
    "2. The notebook will be graded (for graphs and free response questions).\n",
    "\n",
    "\n",
    "**Do not change the function names in the `*.py` file**\n",
    "- The functions in the `*.py` file are how your assignment is graded, and they are graded by their name. The dictionary at the end of the file (`GRADED FUNCTIONS`) contains the \"grading list\". The final function in the file allows your doctests to check that all the necessary functions exist.\n",
    "- If you changed something you weren't supposed to, just use git to revert!\n",
    "\n",
    "**Tips for working in the Notebook**:\n",
    "- The notebooks serve to present you the questions and give you a place to present your results for later review.\n",
    "- The notebook on *HW assignments* are not graded (only the `.py` file).\n",
    "- Notebooks for PAs will serve as a final report for the assignment, and contain conclusions and answers to open ended questions that are graded.\n",
    "- The notebook serves as a nice environment for 'pre-development' and experimentation before designing your function in your `.py` file.\n",
    "\n",
    "**Tips for developing in the .py file**:\n",
    "- Do not change the function names in the starter code; grading is done using these function names.\n",
    "- Do not change the docstrings in the functions. These are there to tell you if your work is on the right track!\n",
    "- You are encouraged to write your own additional functions to solve the HW! \n",
    "    - Developing in python usually consists of larger files, with many short functions.\n",
    "    - You may write your other functions in an additional `.py` file that you import in `hw0X.py` (much like we do in the notebook).\n",
    "- Always document your code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hw04 as hw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Login time questions\n",
    "\n",
    "Imagine that you own an online store and you'd like to monitor the visits to your site. You've collected some data that you store in login_table.csv. It contains the information about different login dates and times for different users. Some users are unique, while some visited your store multiple times.\n",
    "\n",
    "You need to answer a few questions below in order to understand the login patters of your users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1**\n",
    "\n",
    "Write a function `earliest_login` which takes in a dataframe like `login` and outputs a dataframe, indexed by `Login ID`, of the earliest login time for each user.\n",
    "\n",
    "Note, I ask about the **time**, not the earliest date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "login = pd.read_csv('data/login_table.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2**\n",
    "\n",
    "As a site owner, you would like to see how often users return to your site. You've noticed that there are users who have several logins and users who logged in only once. First you need to filter out the users who visited your site only once.\n",
    "\n",
    "Now, when you have only users with multiple logins, you are interested in finding the smallest amount of time elapsed between two consecutive logins for each user.\n",
    "\n",
    "Write a function `smallest_ellapsed` which takes in a dataframe like `login` but without unique IDs and outputs a dataframe, indexed by Login ID, containing the smallest time elapsed for each user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3**\n",
    "\n",
    "Another important piece of information that you might be interested in: what day of the week each user had the most number of logins? What is the most popular day for logins?\n",
    "\n",
    "Write a function `day_of_week` which takes in a dataframe like `login` and returns a tuple of two items: \n",
    "- a series, indexed by Login ID, containing the busiest day for each user. The day should be represented as \"Monday\", \"Tuesday\" etc/ If a given Login ID has multiple \"busiest days\", then return them in a sorted list.\n",
    "- the most popular day overall, as a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4**\n",
    "\n",
    "For each user, determine what time of day most of the logins happen. Label each login as happening:\n",
    "- At `Night`, if the hour of the login is between 12:00 AM and 5:59 AM\n",
    "- In the `Morning` if the hour of the login is between 6:00 AM and 11:59 AM\n",
    "- In the `Afternoon` if the hour of the login is between 12:00 PM and 5:59 PM\n",
    "- In the `Evening` if the time of login is between 6:00 PM and 11:59 PM.\n",
    "\n",
    "For example, time `05:37:45` is `Night`.\n",
    "\n",
    "Write a function `day_time`, which takes in a dataframe like `login` and returns a copy of the given dataframe plus additional column that indicates whether a given time belongs to the \"Night\", \"Morning\", \"Afternoon\", or \"Evening\" category.\n",
    "\n",
    "*Hint*: I found function `pd.cut` to be useful here. You can use any approach you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Probabilities\n",
    "\n",
    "**Question 5**\n",
    "\n",
    "You are given a large text and you have a small text snippet. For simplicity, we will use a snippet with just two words in it. You'd like to find the probability that this snippet occurs in a given text. \n",
    "\n",
    "In order to do that, you need to calculate a bigram probability. (A 'bigram' is a sequence of two adjacent words in a a string). The probability of a bigram appear in a text is given by:\n",
    "\n",
    "$$\n",
    "P(w_i | w_{i-1} ) = \\frac{ count( w_{i-1}, w_i )} {count ( w_{i-1})}\n",
    "$$\n",
    "\n",
    "In other words:\n",
    "\n",
    "The probability that ${\\rm word}_{i-1}$ is followed by word$_i$ equals the number of times we saw ${\\rm word}_{i-1}$ followed by ${\\rm word}_i$ divided by number of times we saw ${\\rm word}_{i-1}$.\n",
    "\n",
    "For example, you have the following text:\n",
    "\n",
    "```\n",
    "Humpty Dumpty sat on a wall\n",
    "Humpty Dumpty had a great fall\n",
    "All the king's horses and all the king's men\n",
    "Could not put Humpty together again\n",
    "```\n",
    "\n",
    "Then the probability that `Dumpty` is followed by `sat` is\n",
    "\n",
    "$$\n",
    "P( `sat` | `Dumpty` ) = \\frac{\\mbox{Number of time `sat` follow `Dumpty`} }{ \\mbox{Number of time we saw `Dumpty`}} = 1/2.\n",
    "$$\n",
    "\n",
    "\n",
    "You need to write a function that takes in a  text, a list of snippets (each snippet has two words in it) and outputs the table with probabilities for each snippet. One column outputs the probabilities without smoothing and another column outputs the probabilities with additive smoothing. \n",
    "\n",
    "* Convert the text to the lower case first.\n",
    "* Use $\\alpha = 1$ as your smoothing factor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open('tomsawyer.txt').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Data\n",
    "\n",
    "**Question 6**\n",
    "\n",
    "In the following questions, you are provided with two datasets and need to assess the validity of the claims about missingness in each of them. To answer the questions, use the definitions of different types of missingness, techniques of assesssing them, and the content of the given datasets. *More than one answer may be correct*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset `payments` contains payment information for purchases on an ecommerce website. The dataset has credit card payment information and date of birth of the shopper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payments = pd.read_csv('data/payment.csv')\n",
    "payments.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create a function `payments_missingness` of zero variables, which returns a list of integers. That list should contain any answer given below that is both reasonable with respect to the data generating process and consistent with the data in `payments`. \n",
    "    1. The `credit_card_number` field is MAR, as younger people less likely to pay with cc.\n",
    "    2. The `credit_card_number` field is MAR: A credit card number is less likely to be reported for certain credit card companies due to differences in processing.\n",
    "    3. The `credit_card_number` field is MCAR: The likelihood of a credit card number being missing doesn't depend on the values of any other field.\n",
    "    4. The `credit_card_number` field is NMAR: a credit card number ending in certain 4-digit suffixes are more likely to be missing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The (fictional) dataset `cars` contains a list of records for parking tickets given out on campus. It contains the car's 'vehicle identification number' registered with the department of motor vehicles, the car's make and year, as well as the color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars = pd.read_csv('data/cars.csv')\n",
    "cars.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create a function `car_missingness` of zero variables, which returns a list of integers. That list should contain any answer given below that is both reasonable with respect to the data generating process and consistent with the data in `cars`.\n",
    "\n",
    "    1. `car_year` may be NMAR, as older cars are less likely to have a complete record.\n",
    "    2. `car_color` is likely not recorded in the registration information.\n",
    "    3. `car_make` is MD: the VIN number determines when it's missing.\n",
    "    4. `car_color` is likely MCAR; its recording is not influenced by the value of any other field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SDPD Vehicle Stops\n",
    "\n",
    "Download the SDPD vehicle stops data and run the cleaning functions below. Be sure you understand what the cleaning functions are doing and why!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaned stops data\n",
    "# DO NOT TOUCH THIS CELL!\n",
    "\n",
    "clean_dict = {\n",
    "    'sd_resident': {'Y': 1, 'N': 0}, \n",
    "    'searched': {'Y': 1, 'N': 0, 'n': 0}\n",
    "}\n",
    "\n",
    "stops = (\n",
    "    pd.read_csv('http://seshat.datasd.org/pd/vehicle_stops_2016_datasd.csv')\n",
    "    .replace(clean_dict)\n",
    ")\n",
    "\n",
    "stops['subject_age'] = pd.to_numeric(stops['subject_age'], errors='coerce')\n",
    "stops['searched'] = pd.to_numeric(stops['searched'], errors='coerce')\n",
    "stops['sd_resident'] = pd.to_numeric(stops['sd_resident'], errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 7**\n",
    "First, we would like to analyze the SDPD vehicle stops data to understand the age differential in traffic stops across the hours of the day. In doing this, drop any rows where the timestamp field isn't valid (convince yourself this isn't a big deal). We'll do this in the following steps:\n",
    "\n",
    "1. Traffic stop times are not recorded exactly when given, which we'll verify. Create a function `mins_after_hour` which takes in a dataframe like `stops` and outputs a series, indexed by minutes after the hour, that give the percentage of stops that occurred that many minutes after the hour. Verify that traffic stops are often recorded as happening exactly *on the hour*. Plot the distribution.\n",
    "\n",
    "2. Since we cannot trust exactly when a traffic stop occurs from the dataset, it makes sense to smooth the the traffic stops data. Using this smoothed data, we are going to compute the average age of a stop by hour of the day, to assess which the age profile of stopped drivers through the day.\n",
    "    - Create a function `avg_age_per_hour` which takes in the `stops` data, computes the average age of stops over a 3 hour rolling window, and returns the average age of a driver for each hour of the day.\n",
    "\n",
    "At what time of day is the average age the oldest? the youngest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 8**\n",
    "\n",
    "Next, we'd like to understand the rate at which people of different ethinicities were searched in the stops data. However, this analysis is complicated by the fact that certain ethnicities appear very rarely; it's difficult to ascertain if these rates reflect the 'true' rate, or are dominated by chance.\n",
    "\n",
    "* First plot the search rates and the counts. Create a function `plot_search` that takes in a dataframe like stops and returns an array of two `matplotlib.axes` subplots that depicts two vertically stacked plots:\n",
    "    1. the top displaying a bar chart of the number of traffic stops per `subject_race`, and\n",
    "    2. the bottom displaying a bar chart of search rates by `subject_race`.\n",
    "\n",
    "Your plot should have the general form of the plot below:\n",
    "![](./search_rate.png)\n",
    "\n",
    "Take note of the relationship between the two. For those `subject_race` with abnormally high search rates, what do we notice about the number of traffic stops?\n",
    "\n",
    "Next we will smooth the search rates using conditional additive smoothing, which is given by the formula below (see lecture for an explanation of the notation):\n",
    "\n",
    "$$ p_i = \\frac{b_i + \\alpha\\cdot(b/N)}{x_i + \\alpha} \\qquad {\\rm (smoothed)}$$\n",
    "\n",
    "Notice that these smoothed probabilities interpolate between the search rate among a group and the overall search rate.\n",
    "\n",
    "* Create a function called `smoothed_search_rates` that takes in `stops` data and a parameter `alpha` and returns a Series indexed by `subject_race` of the smoothed search rates for each value of `subject_race`.\n",
    "\n",
    "Compare the search rates with the smoothed search rates for `alpha=1000`. Note `alpha` has the same meaning as in regular additive smoothing (do you see why in the formula?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will impute the missing values of the `sd_resident` column. Unlike `subject_age`, which is a quantitative column, the `sd_resident` column is binary. As such it doesn't make sense to impute the column with its mean, which is a number between 0 and 1. As noted in lecture, we can instead probabilitistically impute the `sd_resident` column, drawing from a distribution with the desired mean. \n",
    "\n",
    "**Question 9 **\n",
    "\n",
    "First we will assess whether `sd_resident` is MAR, and which columns it likely depends on. \n",
    "\n",
    "* Create a function `sd_res_missingness` that takes in the `stops` dataset and a column `col`, and returns the Total Variational Distance between the following two distributions:\n",
    "    - the empirical distribution of `col` conditional on `sd_resident` being *null*, and\n",
    "    - the empirical distribution of `col` conditional on `sd_resident` being *non-null*.\n",
    "\n",
    "For which columns do these distribution have the greatest difference? (among `['stop_cause', 'service_area', 'subject_race', 'subject_sex', 'subject_age']`). Verify the top two for yourself -- plotting the distributions as bar charts is a nice was to understand the differences.\n",
    "\n",
    "* Create a function `fill_sd_res_mcar` that takes in the stops data and outputs a column (the 'imputed `sd_resident`') that satisfies the following conditions:\n",
    "    - The output column of `fill_sd_res_mcar` is either 0 or 1,\n",
    "    - non-null entries of `sd_resident` remain unchanged,\n",
    "    - the output has *almost* the same mean as the unimputed `sd_resident` column.\n",
    "    - That is, the column is the result of (MCAR) \"imputation from a distribution\". (See lecture 07).\n",
    "\n",
    "\n",
    "* Assume (for the sake of simplicity) that `sd_resident` is MAR, conditional only on the column `service_area`. Create a function `fill_sd_res_mar` that takes in a row of the stops data and outputs a value (the 'imputed `sd_resident`') that satisfies the following conditions:\n",
    "    - The output column of `fill_sd_res_mar` is either 0 or 1,\n",
    "    - non-null entries of `sd_resident` remain unchanged,\n",
    "    - the output has *almost* the same mean as the unimputed `sd_resident` column.\n",
    "    - The condition above remains true when grouping by `service_area`.\n",
    "    - That is, the column is the result of (MAR) \"imputation from a distribution\" conditional on `service_area`. (See lecture 07).\n",
    "    \n",
    "Verify for yourself the difference in the two imputation methods on the `sd_resident column`. What are the means of the two imputed columns? What about across different values of `service_area`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Visits (Extra Credit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 10**\n",
    "\n",
    "Given below are two datasets from an ecommerce company selling DVDs of 50 movies on sale. The company paid for an ad campaign on a number of websites. When a customer clicks on the advertisement displaying a movie title, they arrive at the sale page of the movie. The company has the following datasets from a single day's activity on their website:\n",
    "\n",
    "1. `purch` contains information about the purchase of the movie: time of purchase, the movie bought, the price, and the email address of the purchaser.\n",
    "2. `webvisits` contains information about when a visitor came from one of the paid advertisement: which website they came from, which movie the advertisement was featuring, as well as other logging data for the visit.\n",
    "\n",
    "To assess the effectiveness of the campaign, the company would like to know which purchases (in `purch`) came from traffic via the ad-campaign (in `webvisits`). Since there is no unique key between the two datasets, an approximation will have to suffice. Use the following approach to join the data:\n",
    "\n",
    "A visit should be paired with a purchase only when:\n",
    "- they are for the same movie,\n",
    "- the visit and the purchase are less than 5 minutes apart,\n",
    "- every purchase (uniquely identified by email) should only appear once in the joined data,\n",
    "- a single visit should not be associated to multiple purchases (the visit closest to the purchase should be used).\n",
    "\n",
    "Create a function `join_purch_visits` that takes in two dataframes like `purch` and `webvisits` and outputs the joined data as described above. Every purchase should be present in the output, even if there is no matching visit.\n",
    "\n",
    "Create a function `revenue_by_url` which takes in a joined dataframe and gives the total revenue associated to the visits of each ad campaign (i.e. url).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "purch = pd.read_csv('data/purch.csv')\n",
    "webvisits = pd.read_csv('data/webvisits.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "purch.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "webvisits.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations, you're done with the homework\n",
    "\n",
    "### Now, run your doctests and upload `hw04.py` to GradeScope.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
